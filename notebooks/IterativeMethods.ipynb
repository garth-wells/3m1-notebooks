{
 "metadata": {
  "name": "",
  "signature": "sha256:1c40a00661cb412bec49f80801583404e7f7c2cef3aec1adcdf267b0fa4befc8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Iterative methods (under development)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We explore in this notebook some iterative techniques for linear algebra problems.\n",
      "\n",
      "The implementations are written to expose how the methods work. The implementations are not optimised. Production-level implementations typically involve some specialised optimisations.\n",
      "\n",
      "This notebook illustrates:\n",
      "\n",
      "- Power iteration for eigenvalue problems\n",
      "- Stationary iterative methods\n",
      "- Conjugate gradient method"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stiffness matrix generator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The examples will use the so-called *stiffness matrix* (this matrix comes up in the module 3D7) but of different sizes. The stiffness matrix is a symmetric positive definite (SPD) matrix.\n",
      "We will start with a function to build a $n \\times n$ stiffness matrix. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "def create_stiffness_matrix(n):\n",
      "    \"Create a stiffness matrix of size n x n\"\n",
      "\n",
      "    # Create a zero matrix\n",
      "    A = np.zeros((n + 1, n + 1))\n",
      "\n",
      "    # Add components of small matrix k to A\n",
      "    k = np.array([[1.0, -1.0], [-1.0, 1.0]])\n",
      "    for i in range(len(A) - 1):\n",
      "        A[i:i+2, i:i+2] += k\n",
      "\n",
      "    # We will remove the last row and column of the matrix. This will\n",
      "    # make the matrix invetible (SPD)\n",
      "    A = np.delete(A, -1, 0)    \n",
      "    A = np.delete(A, -1, 1)   \n",
      "    return A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Power iteration for the maximum eigenvalue"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A very simple algorithm that you have seen before is power iteration for estimating the maximum (absolute) eigenvalues and corresponding eigenvector of a matrix is *power iteration*.\n",
      "\n",
      "We will create a small stiffness matrix.  The stiffness matrix has eigenvalues that are close, and which get closer for large $n$. To measure the error in the estimated eigenvalues and eigenvectors, we will first add a function to compute the maximum eigenpair of a matrix directly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def max_eigenpair(A, print_values=False):\n",
      "    \"Compute the eigenpair for the largest eigenvalue. Matrix A is assumed symmetric\"\n",
      "    \n",
      "    # Compute eigenpairs\n",
      "    evals, evecs = np.linalg.eigh(A)\n",
      "\n",
      "    # Get index of largest absolute eigenvalue\n",
      "    index = np.argmax(np.abs(evals))\n",
      "\n",
      "    # Get largest eigenvalue and corresponding eigenvector\n",
      "    eval_max = evals[index]\n",
      "    evec_max = evecs[:, index]/np.linalg.norm(evecs[:, index])\n",
      "\n",
      "    if print_values:\n",
      "        print(\"  Largest eigenvalue:        {}\".format(eval_max))\n",
      "\n",
      "        # Get second largest eigenvalue to compare to largest\n",
      "        eval1 = evals[np.argsort(abs(evals))[-2]]\n",
      "        print(\"  Second largest eigenvalue: {}\".format(eval1))\n",
      "    \n",
      "    return eval_max, evec_max"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Computing the largest and second largest eigenvalues for $5 \\times 5$ and $20 \\times 20$ matrices, we see that they get closer as the matrix size increases:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Check largest and second eigenvalues of 5x5 stiffness matrix\n",
      "print(\"Stiffness matrix 5 x 5:\")\n",
      "max_eigenpair(create_stiffness_matrix(5), print_values=True);\n",
      "\n",
      "# Check largest and second eigenvalues of 10x10 stiffness matrix\n",
      "print(\"Stiffness matrix 20 x 20:\")\n",
      "max_eigenpair(create_stiffness_matrix(20), print_values=True);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Stiffness matrix 5 x 5:\n",
        "  Largest eigenvalue:        3.68250706566\n",
        "  Second largest eigenvalue: 2.830830026\n",
        "Stiffness matrix 20 x 20:\n",
        "  Largest eigenvalue:        3.97656084756\n",
        "  Second largest eigenvalue: 3.90679278411\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now apply power iteration to estimate the largest eigenvalue and corresponding eigenvalue. We know that convergence will be poor as the matrix size increases because the  largest and second largest eigenvalues become a close, so we will test with the small $10 \\times 10$ matrix. \n",
      "\n",
      "We perform a power iteration with a random starting vector $\\boldsymbol{u}_{0}$ and 10 iterations. At each iteration we compare the error in the eigenvalue estimated by \n",
      "\n",
      "- Scaling of the solution from one iterate to the next; and \n",
      "- Rayleigh quotient $R = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}/(\\boldsymbol{x}^{T} \\boldsymbol{x})$\n",
      "\n",
      "We also compute the error in eigenvector as $\\| \\boldsymbol{u}_{\\text{exact}} - \\boldsymbol{u}_{k} \\|_{2}$, where $\\boldsymbol{u}_{k}$ is the estimate of the eigenvector at the $k$th iterate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create 10x10 matrix\n",
      "A = create_stiffness_matrix(10)\n",
      "\n",
      "# Get exact eigenpair to compute errors\n",
      "lambda_max, u_max = max_eigenpair(A)\n",
      "\n",
      "# Create random starting vector and normalise\n",
      "np.random.seed(3)\n",
      "u0 = np.random.rand(A.shape[1])\n",
      "u0 = u0/np.linalg.norm(u0)\n",
      "\n",
      "# Perform power iteration\n",
      "for k in range(10):\n",
      "    print(\"Step: {}\".format(k))\n",
      "\n",
      "    # Compute u_{k+1} = A u_{k}\n",
      "    u1 = A.dot(u0)\n",
      "\n",
      "    # Estimate eigenvalue (from scaling of each entry and by Rayleigh quotient)\n",
      "    lambda_est   = np.divide(u1, u0)\n",
      "    rayleigh_est = u1.dot(A.dot(u1))/(u1.dot(u1))\n",
      "\n",
      "    # Normalise estimated eigenvector and assign to u0\n",
      "    u0 = u1/np.linalg.norm(u1)\n",
      "\n",
      "    # Print errors in eigenvalue\n",
      "    print(\"  Relative errors\")    \n",
      "    print \"    lambda (scaling):     \", np.abs(lambda_max - np.average(lambda_est))/lambda_max    \n",
      "    print \"    lambda (Rayleigh):    \", np.abs(lambda_max - rayleigh_est)/lambda_max    \n",
      "\n",
      "    # Get signs on eigenvectors (could be pointing in opposite directions) and print error\n",
      "    dir0, dir_est = abs(u_max[0])/u_max[0], abs(u0[0])/u0[0]\n",
      "    print \"    u (l2):               \", np.linalg.norm(dir0*u_max - dir_est*u0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Step: 0\n",
        "  Relative errors\n",
        "    lambda (scaling):      1.384646117\n",
        "    lambda (Rayleigh):     0.247897492642\n",
        "    u (l2):                0.893863604888\n",
        "Step: 1\n",
        "  Relative errors\n",
        "    lambda (scaling):      0.187638838776\n",
        "    lambda (Rayleigh):     0.110661729285\n",
        "    u (l2):                0.695739955198\n",
        "Step: 2\n",
        "  Relative errors\n",
        "    lambda (scaling):      1.11718386833\n",
        "    lambda (Rayleigh):     0.0558908010444\n",
        "    u (l2):                0.571889091588\n",
        "Step: 3\n",
        "  Relative errors\n",
        "    lambda (scaling):      0.0802417631464\n",
        "    lambda (Rayleigh):     0.0338304270081\n",
        "    u (l2):                0.490190671581\n",
        "Step: 4\n",
        "  Relative errors\n",
        "    lambda (scaling):      0.00153281444\n",
        "    lambda (Rayleigh):     0.0228823303756\n",
        "    u (l2):                0.429602241986\n",
        "Step: 5\n",
        "  Relative errors\n",
        "    lambda (scaling):      0.016645293525\n",
        "    lambda (Rayleigh):     0.0164100420122\n",
        "    u (l2):                0.381398286013\n",
        "Step: 6\n",
        "  Relative errors\n",
        "    lambda (scaling):      0.0195975245933\n",
        "    lambda (Rayleigh):     0.0121854752746\n",
        "    u (l2):                0.341676359043\n",
        "Step: 7\n",
        "  Relative errors\n",
        "    lambda (scaling):      0.0188494617942\n",
        "    lambda (Rayleigh):     0.00927466048591\n",
        "    u (l2):                0.308281315862\n",
        "Step: 8\n",
        "  Relative errors\n",
        "    lambda (scaling):      0.0167749266041\n",
        "    lambda (Rayleigh):     0.0072004750549\n",
        "    u (l2):                0.279805580829\n",
        "Step: 9\n",
        "  Relative errors\n",
        "    lambda (scaling):      0.0142962215653\n",
        "    lambda (Rayleigh):     0.00568589438737\n",
        "    u (l2):                0.255239572853\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some observations:\n",
      "\n",
      "- The first observation is that the method is slow to converge. It takes close to 100 iterations to get a good estimate of the eigenvector\n",
      "- The error in the estimate of the largest eigenvalue via the Rayleigh quotient decreases monotonically\n",
      "- For a given error in the estimated eigenvector, the error in the estimated eigenvalue via the Rayleigh quotient is much smaller."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stationary iterative methods for $\\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now looks at methods for the very important problem of finding (approximate) solutions to $\\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b}$. We looks first at simple stationary methods, and then the more elaborate conjugate gradient method."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Stationary methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We consider methods that involve splitting $\\boldsymbol{A}$ such that $\\boldsymbol{A} = \\boldsymbol{N} - \\boldsymbol{P}$, and then solving\n",
      "\n",
      "$$\n",
      "\\boldsymbol{x}_{k+1} = \\boldsymbol{N}^{-1} \\left(\\boldsymbol{b} \n",
      "+ \\boldsymbol{P}\\boldsymbol{x}_{k} \\right)\n",
      "$$\n",
      "\n",
      "We assume that $\\boldsymbol{A}$ is hard/expensive to solve, and it is split such that $\\boldsymbol{N}$ is easy/cheap to solve.\n",
      "\n",
      "We will experiment with the Jacobi and Gauss-Seidel methods. The implementations have been designed for readability, not performance.\n",
      "\n",
      "We will test using a $50 \\times 50$ stiffness matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = create_stiffness_matrix(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Jacobi method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A very simple method is the Jacobi method, in which $\\boldsymbol{N} = \\text{diag}(\\boldsymbol{A})$ and which is trivial to invert. The method becomes: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\boldsymbol{x}_{k+1} = \\text{diag}(\\boldsymbol{A})^{-1} \\boldsymbol{b} \n",
      "+ (\\boldsymbol{I} - \\text{diag}(\\boldsymbol{A})^{-1} \\boldsymbol{A}) \\boldsymbol{x}_{k}\n",
      "$$\n",
      "\n",
      "To implement this, we first create the $\\boldsymbol{N} = \\boldsymbol{D} = \\text{diag}(\\boldsymbol{A})$ matrix and its inverse: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = np.diag(np.diag(A))\n",
      "Dinv = np.diag(1.0/np.diag(A))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have seen in the lectures that the stationary methods will only converge if \n",
      "$\\rho(\\boldsymbol{N}^{-1} \\boldsymbol{P}) < 1$, where $\\rho$ is the spectral radius (recall that spectral radius of a matrix is the largest absolute eigenvalue). Let's compute the spectral radius for this problem:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = D\n",
      "P = N - A\n",
      "evals = np.linalg.eigvals(Dinv.dot(P))\n",
      "print \"Spectral radius (rho) is: {}\".format(np.max(abs(evals)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Spectral radius (rho) is: 0.999506560366\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The largest eigenvalue is less that one (just!), so we can expect the Jacobi method to converge. However, it is close to one so we expect the convergence to be slow. \n",
      "\n",
      "**Experiment:** compare the spectral radius for different size matrices.\n",
      "\n",
      "Let's try solving $\\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b}$ with Jacobi's method using 15 iterations and with $\\boldsymbol{b} = \\boldsymbol{1}$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "I = np.identity(A.shape[0])\n",
      "\n",
      "b = np.ones(A.shape[1])\n",
      "x = np.zeros(A.shape[1])\n",
      "r0_norm = np.linalg.norm(b - A.dot(x), 2)\n",
      "for k in range(15):\n",
      "    x = Dinv.dot(b) + (I - Dinv.dot(A)).dot(x)\n",
      "    r = b - A.dot(x)\n",
      "    print(\"Step: {}, relative norm of residual (l2): {}\".format(k + 1, np.linalg.norm(r, 2)/r0_norm))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Step: 1, relative norm of residual (l2): 0.997496867163\n",
        "Step: 2, relative norm of residual (l2): 0.98931794687\n",
        "Step: 3, relative norm of residual (l2): 0.988685996664\n",
        "Step: 4, relative norm of residual (l2): 0.98325829516\n",
        "Step: 5, relative norm of residual (l2): 0.98274170055\n",
        "Step: 6, relative norm of residual (l2): 0.978389540904\n",
        "Step: 7, relative norm of residual (l2): 0.977930292122\n",
        "Step: 8, relative norm of residual (l2): 0.974191966603\n",
        "Step: 9, relative norm of residual (l2): 0.973771323679\n",
        "Step: 10, relative norm of residual (l2): 0.970441874112\n",
        "Step: 11, relative norm of residual (l2): 0.970050121468\n",
        "Step: 12, relative norm of residual (l2): 0.967017668818\n",
        "Step: 13, relative norm of residual (l2): 0.966648783738\n",
        "Step: 14, relative norm of residual (l2): 0.963844453337\n",
        "Step: 15, relative norm of residual (l2): 0.963494333737\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that the residual is decreasing, but very slowly. This is to be expected because $\\rho(\\boldsymbol{N}^{-1} \\boldsymbol{P})$ is very close to one."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gauss-Seidel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the Gauss-Seidel, method, $\\boldsymbol{N}$ is the lower triangular part of $\\boldsymbol{A}$ and $\\boldsymbol{P}$ is the strictly upper triangular part of $\\boldsymbol{A}$ ($\\boldsymbol{P}$ is zero on the diagonal). Solving  $\\boldsymbol{N} \\boldsymbol{y} = \\boldsymbol{f}$ is then analogous to the forward substitution step in a LU solver.\n",
      "\n",
      "From the above general expression for stationary methods, we multiply both sides by $\\boldsymbol{N}$:\n",
      "\n",
      "$$\n",
      "\\boldsymbol{N} \\boldsymbol{x}_{k+1} = \\boldsymbol{b} + \\boldsymbol{P}\\boldsymbol{x}_{k}\n",
      "$$\n",
      "\n",
      "We will solve our problem using this formulation. We first form $\\boldsymbol{N}$ and $\\boldsymbol{P}$ "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Lower-triangular part of A\n",
      "N = np.tril(A)\n",
      "\n",
      "# Compute P\n",
      "P = N - A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now check the spectral radius for the Gauss-Seidel case:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evals = np.linalg.eigvals(np.linalg.inv(N).dot(P))\n",
      "print \"Largest eigenvalue (rho) is: {}\".format(np.max(abs(evals)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Largest eigenvalue (rho) is: 0.999013364214\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The spectral radius is very slightly smaller that for the Jacobi case, but still very close to one so we cannot expect good convergence.\n",
      "\n",
      "Now the solver. We use SciPy for the forward substitution step involving $\\boldsymbol{N}$ and perform 15 iterations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.linalg as LA\n",
      "\n",
      "b = np.ones(A.shape[1])\n",
      "x = np.zeros(A.shape[1])\n",
      "r0_norm = np.linalg.norm(b - A.dot(x), 2)\n",
      "for k in range(15):\n",
      "    c = b + P.dot(x)\n",
      "    x = LA.solve_triangular(N, c, lower=True)\n",
      "\n",
      "    # Compute residual to monitor convergence\n",
      "    r = b - A.dot(x)\n",
      "    print(\"Step: {}, relative norm of residual (l2): {}\".format(k + 1, np.linalg.norm(r, 2)/r0_norm))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Step: 1, relative norm of residual (l2): 0.989949493661\n",
        "Step: 2, relative norm of residual (l2): 0.982344135219\n",
        "Step: 3, relative norm of residual (l2): 0.976761229779\n",
        "Step: 4, relative norm of residual (l2): 0.972151287095\n",
        "Step: 5, relative norm of residual (l2): 0.968122906913\n",
        "Step: 6, relative norm of residual (l2): 0.964493072659\n",
        "Step: 7, relative norm of residual (l2): 0.961159037043\n",
        "Step: 8, relative norm of residual (l2): 0.958056014751\n",
        "Step: 9, relative norm of residual (l2): 0.955139940101\n",
        "Step: 10, relative norm of residual (l2): 0.952379199662\n",
        "Step: 11, relative norm of residual (l2): 0.949750191688\n",
        "Step: 12, relative norm of residual (l2): 0.947234739558\n",
        "Step: 13, relative norm of residual (l2): 0.944818490486\n",
        "Step: 14, relative norm of residual (l2): 0.942489875579\n",
        "Step: 15, relative norm of residual (l2): 0.940239407731\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see from the decrease in the residual that the Gauss-Seidel method converges somewhat faster than the Jacobi method, but it is still far too slow to be of any practical use on its own."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Applications of stationary methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stationary methods are of limited use of their own as they tend to converge very slowly. They are however very useful in combination with other methods, e.g. as preconditoners in more sophisticated iterative methods and as 'smoothers' in multigrid methods."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Conjugate gradient method for $\\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now look at one of the most important algorithms for solving $\\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b}$, the conjugate gradient (CG) method. The CG method is applicable to symmetric positive-definite (SPD) matrices.\n",
      "\n",
      "The CG methods is technically a direct method since in exact arithmetic is solves an $n \\times n$ system in $k$ iterations, where $k$ is the number of distinct eigenvalues of $\\boldsymbol{A}$. In the worst case it requires $n$ iterations.\n",
      "\n",
      "In practice, the CG methods is used as an iterative method to find approximate solutions. If solving an $n \\times n$ system required $n$ steps it would generally not be competitive with other methods, and round-off error can spoil the party. \n",
      "\n",
      "The error in the CG method decreases monotonically in the energy norm, i.e. if $\\boldsymbol{x}$ is the exact solution then:\n",
      "\n",
      "$$\n",
      "(\\boldsymbol{x} - \\boldsymbol{x}_{k+1})^{T} \\boldsymbol{A} (\\boldsymbol{x} - \\boldsymbol{x}_{k+1}) < \n",
      "(\\boldsymbol{x} - \\boldsymbol{x}_{k})^{T} \\boldsymbol{A} (\\boldsymbol{x} - \\boldsymbol{x}_{k}) \n",
      "$$\n",
      "\n",
      "This implies that if we stop iterating before the residual is (almost) zero, we will have a better solution than we started with.\n",
      "\n",
      "Below we solve the $50 \\times 50$ stiffness matrix problem with the conjugate gradient method. The iterations terminate once the residual drops below a specified threshold. The solver prints at each iteration the relative error in the solution in the $\\boldsymbol{A}$-norm and the relative residual in the $l_{2}$ norm."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# RHS vector\n",
      "b = np.ones(A.shape[1])\n",
      "\n",
      "# Initial guess (zero vector)\n",
      "x0 = np.zeros(A.shape[1])\n",
      "\n",
      "# Compute exact solution (to use in computing error at each iterate)\n",
      "x_exact = np.linalg.solve(A, b)\n",
      "e = x_exact - x0\n",
      "e0_norm = np.sqrt(e.dot(A.dot(e)))\n",
      "print(\"Initial error (A-norm): {}\".format(e.dot(A.dot(e)))) \n",
      "\n",
      "# Convergence tolerance to exit solver\n",
      "tolerance = 1.0e-9\n",
      "\n",
      "# Create starting vectors\n",
      "r0 = b - A.dot(x0)\n",
      "r0_norm = np.linalg.norm(r0)\n",
      "p0 = r0.copy()\n",
      "\n",
      "# Start iterations    \n",
      "for k in range(200):\n",
      "    print(\"Step: {}\".format(k))\n",
      "\n",
      "    alpha = r0.dot(r0)/(p0.dot(A.dot(p0)))\n",
      "    x1 = x0 + alpha*p0\n",
      "    r1 = r0 - alpha*A.dot(p0)\n",
      "\n",
      "    # Compute error in x (this is for studying the algorithm, and of \n",
      "    # course would never be done in practice)\n",
      "    e = x_exact - x1\n",
      "    e_norm = np.sqrt(e.dot(A.dot(e)))\n",
      "    print(\"  Relative error in x (A-norm): {}\".format(e_norm/e0_norm)) \n",
      "    \n",
      "    # Compute norm of residual and check for converge\n",
      "    r_norm = np.linalg.norm(r1) \n",
      "    print(\"  Relative residual (l2-norm):  {}\".format(r_norm/r0_norm)) \n",
      "    if r_norm < tolerance:\n",
      "        break\n",
      "\n",
      "    beta = r1.dot(r1)/r0.dot(r0)\n",
      "    p1 = r1 + beta*p0    \n",
      "    \n",
      "    # Update for next step\n",
      "    p0, r0, x0 = p1, r1, x1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initial error (A-norm): 42925.0\n",
        "Step: 0\n",
        "  Relative error in x (A-norm): 0.970442621576\n",
        "  Relative residual (l2-norm):  7.0\n",
        "Step: 1\n",
        "  Relative error in x (A-norm): 0.941182294682\n",
        "  Relative residual (l2-norm):  6.85857127979\n",
        "Step: 2\n",
        "  Relative error in x (A-norm): 0.912222065762\n",
        "  Relative residual (l2-norm):  6.7171422495\n",
        "Step: 3\n",
        "  Relative error in x (A-norm): 0.883565076944\n",
        "  Relative residual (l2-norm):  6.57571288911\n",
        "Step: 4\n",
        "  Relative error in x (A-norm): 0.855214571161\n",
        "  Relative residual (l2-norm):  6.43428317686\n",
        "Step: 5\n",
        "  Relative error in x (A-norm): 0.827173897654\n",
        "  Relative residual (l2-norm):  6.29285308902\n",
        "Step: 6\n",
        "  Relative error in x (A-norm): 0.799446517913\n",
        "  Relative residual (l2-norm):  6.15142259969\n",
        "Step: 7\n",
        "  Relative error in x (A-norm): 0.772036012088\n",
        "  Relative residual (l2-norm):  6.00999168053\n",
        "Step: 8\n",
        "  Relative error in x (A-norm): 0.744946085927\n",
        "  Relative residual (l2-norm):  5.86856030045\n",
        "Step: 9\n",
        "  Relative error in x (A-norm): 0.718180578295\n",
        "  Relative residual (l2-norm):  5.72712842531\n",
        "Step: 10\n",
        "  Relative error in x (A-norm): 0.691743469339\n",
        "  Relative residual (l2-norm):  5.58569601751\n",
        "Step: 11\n",
        "  Relative error in x (A-norm): 0.665638889371\n",
        "  Relative residual (l2-norm):  5.44426303553\n",
        "Step: 12\n",
        "  Relative error in x (A-norm): 0.639871128551\n",
        "  Relative residual (l2-norm):  5.30282943343\n",
        "Step: 13\n",
        "  Relative error in x (A-norm): 0.614444647475\n",
        "  Relative residual (l2-norm):  5.16139516023\n",
        "Step: 14\n",
        "  Relative error in x (A-norm): 0.589364088761\n",
        "  Relative residual (l2-norm):  5.0199601592\n",
        "Step: 15\n",
        "  Relative error in x (A-norm): 0.564634289774\n",
        "  Relative residual (l2-norm):  4.87852436706\n",
        "Step: 16\n",
        "  Relative error in x (A-norm): 0.540260296634\n",
        "  Relative residual (l2-norm):  4.73708771293\n",
        "Step: 17\n",
        "  Relative error in x (A-norm): 0.516247379672\n",
        "  Relative residual (l2-norm):  4.59565011723\n",
        "Step: 18\n",
        "  Relative error in x (A-norm): 0.492601050538\n",
        "  Relative residual (l2-norm):  4.45421149026\n",
        "Step: 19\n",
        "  Relative error in x (A-norm): 0.469327081196\n",
        "  Relative residual (l2-norm):  4.31277173057\n",
        "Step: 20\n",
        "  Relative error in x (A-norm): 0.446431525076\n",
        "  Relative residual (l2-norm):  4.17133072292\n",
        "Step: 21\n",
        "  Relative error in x (A-norm): 0.423920740716\n",
        "  Relative residual (l2-norm):  4.02988833592\n",
        "Step: 22\n",
        "  Relative error in x (A-norm): 0.40180141828\n",
        "  Relative residual (l2-norm):  3.88844441904\n",
        "Step: 23\n",
        "  Relative error in x (A-norm): 0.380080609419\n",
        "  Relative residual (l2-norm):  3.74699879904\n",
        "Step: 24\n",
        "  Relative error in x (A-norm): 0.35876576103\n",
        "  Relative residual (l2-norm):  3.60555127546\n",
        "Step: 25\n",
        "  Relative error in x (A-norm): 0.337864753607\n",
        "  Relative residual (l2-norm):  3.46410161514\n",
        "Step: 26\n",
        "  Relative error in x (A-norm): 0.31738594501\n",
        "  Relative residual (l2-norm):  3.32264954517\n",
        "Step: 27\n",
        "  Relative error in x (A-norm): 0.297338220672\n",
        "  Relative residual (l2-norm):  3.18119474412\n",
        "Step: 28\n",
        "  Relative error in x (A-norm): 0.277731051528\n",
        "  Relative residual (l2-norm):  3.03973683071\n",
        "Step: 29\n",
        "  Relative error in x (A-norm): 0.258574561253\n",
        "  Relative residual (l2-norm):  2.89827534924\n",
        "Step: 30\n",
        "  Relative error in x (A-norm): 0.239879604824\n",
        "  Relative residual (l2-norm):  2.75680975042\n",
        "Step: 31\n",
        "  Relative error in x (A-norm): 0.221657860989\n",
        "  Relative residual (l2-norm):  2.61533936612\n",
        "Step: 32\n",
        "  Relative error in x (A-norm): 0.203921941968\n",
        "  Relative residual (l2-norm):  2.47386337537\n",
        "Step: 33\n",
        "  Relative error in x (A-norm): 0.186685524743\n",
        "  Relative residual (l2-norm):  2.33238075794\n",
        "Step: 34\n",
        "  Relative error in x (A-norm): 0.169963509736\n",
        "  Relative residual (l2-norm):  2.19089023002\n",
        "Step: 35\n",
        "  Relative error in x (A-norm): 0.153772214659\n",
        "  Relative residual (l2-norm):  2.04939015319\n",
        "Step: 36\n",
        "  Relative error in x (A-norm): 0.138129614247\n",
        "  Relative residual (l2-norm):  1.90787840283\n",
        "Step: 37\n",
        "  Relative error in x (A-norm): 0.123055640828\n",
        "  Relative residual (l2-norm):  1.76635217327\n",
        "Step: 38\n",
        "  Relative error in x (A-norm): 0.108572567114\n",
        "  Relative residual (l2-norm):  1.62480768093\n",
        "Step: 39\n",
        "  Relative error in x (A-norm): 0.0947055025188\n",
        "  Relative residual (l2-norm):  1.48323969742\n",
        "Step: 40\n",
        "  Relative error in x (A-norm): 0.0814830502507\n",
        "  Relative residual (l2-norm):  1.3416407865\n",
        "Step: 41\n",
        "  Relative error in x (A-norm): 0.0689381987546\n",
        "  Relative residual (l2-norm):  1.2\n",
        "Step: 42\n",
        "  Relative error in x (A-norm): 0.0571095668067\n",
        "  Relative residual (l2-norm):  1.05830052443\n",
        "Step: 43\n",
        "  Relative error in x (A-norm): 0.0460432047489\n",
        "  Relative residual (l2-norm):  0.916515138991\n",
        "Step: 44\n",
        "  Relative error in x (A-norm): 0.0357953153506\n",
        "  Relative residual (l2-norm):  0.774596669241\n",
        "Step: 45\n",
        "  Relative error in x (A-norm): 0.0264365924195\n",
        "  Relative residual (l2-norm):  0.632455532034\n",
        "Step: 46\n",
        "  Relative error in x (A-norm): 0.0180596307295\n",
        "  Relative residual (l2-norm):  0.489897948557\n",
        "Step: 47\n",
        "  Relative error in x (A-norm): 0.0107926936609\n",
        "  Relative residual (l2-norm):  0.346410161514\n",
        "Step: 48\n",
        "  Relative error in x (A-norm): 0.00482663933724\n",
        "  Relative residual (l2-norm):  0.2\n",
        "Step: 49\n",
        "  Relative error in x (A-norm): 0.0\n",
        "  Relative residual (l2-norm):  0.0\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that, as expected, the CG method for an $n \\times n$ matrix solves the problem in $n$ iterations. However if, if we were satisfied with a less accurate solution we could have terminated the iterations sooner.\n",
      "\n",
      "Note that the reduction of the error in $\\boldsymbol{A}$-norm ($\\sqrt{\\boldsymbol{e}^{T}\\boldsymbol{A}\\boldsymbol{e}}$) decreases monotonically. The $l_{2}$-norm of the residual $\\boldsymbol{r}_{k} = \\boldsymbol{b} - \\boldsymbol{A}\\boldsymbol{x}_{k}$ increases in the first step (the relative norm of the residual is greater than one), and then starts to decrease. The monotonic convergence in the $\\boldsymbol{A}$-norm is what we expect from analysis of the CG method.   "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}